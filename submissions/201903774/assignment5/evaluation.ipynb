{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs_1uXhbHZAO"
   },
   "source": [
    "## 2-1. 제목 (Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tjNlJy-OHbje"
   },
   "outputs": [],
   "source": [
    "# Assignment 5 — Model Evaluation (KoBART Summarization)\n",
    "\n",
    "# 이 노트북은 Assignment 5에서 학습한 KoBART 한국어 요약 모델을\n",
    "# Test set에서 평가하고, ROUGE 지표 및 예시 요약 결과를 확인하는 노트북입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMhOxLF9He5F"
   },
   "source": [
    "## 2-2. 환경 설정 & 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysRLiKrfHg7u",
    "outputId": "c01dd821-d9ff-4c21-a900-255b117130fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Mounted at /content/drive\n",
      "MODEL_DIR: /content/drive/MyDrive/boncahier/models/kobart_ko_news\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets sentencepiece accelerate evaluate rouge-score\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# ✅ 학습 때 저장한 모델 경로와 동일하게 설정할 것!\n",
    "MODEL_DIR = \"/content/drive/MyDrive/boncahier/models/kobart_ko_news\"\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)  # 또는 Google Drive/HF Hub 경로로 수정\n",
    "SEED = 42\n",
    "\n",
    "MAX_SOURCE_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# 평가 시 샘플 수 (None이면 전체)\n",
    "MAX_TEST_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgOdtPk_HklF"
   },
   "source": [
    "## 2-3. 데이터 로드 & 동일 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1yFpFJYHoRd",
    "outputId": "9e7f4898-a55f-4d82-d3f3-3467ab3a0c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test size: 2220\n",
      "Subsampled test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Assignment 4에서 저장한 CSV를 이용해 test set 재구성\n",
    "#    - training.ipynb와 동일한 방식으로 split해야 함 (seed 42, 8:1:1)\n",
    "# ============================================================\n",
    "\n",
    "csv_path = \"data/naver_news_summarization_ko.csv\"\n",
    "\n",
    "ko_df = pd.read_csv(csv_path)\n",
    "ko_df = ko_df[[\"document\", \"summary\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "raw_dataset = Dataset.from_pandas(ko_df, preserve_index=False)\n",
    "raw_dataset = raw_dataset.shuffle(seed=SEED)\n",
    "\n",
    "train_valid_test = raw_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "temp = train_valid_test[\"test\"]\n",
    "valid_test = temp.train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_valid_test[\"train\"],\n",
    "    \"validation\": valid_test[\"train\"],\n",
    "    \"test\": valid_test[\"test\"],\n",
    "})\n",
    "\n",
    "test_dataset = dataset_dict[\"test\"]\n",
    "print(\"Full test size:\", len(test_dataset))\n",
    "\n",
    "def maybe_subsample(ds, max_samples, seed=SEED):\n",
    "    if max_samples is None or len(ds) <= max_samples:\n",
    "        return ds\n",
    "    return ds.shuffle(seed=seed).select(range(max_samples))\n",
    "\n",
    "test_dataset = maybe_subsample(test_dataset, MAX_TEST_SAMPLES)\n",
    "print(\"Subsampled test size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D2gkLtUHqmd"
   },
   "source": [
    "## 2-4. 토크나이저 & 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ba4e6cbb86084d4993af25f6db3a022a",
      "35dde34699624b10835d5b4386735bbc",
      "e78a29ebd1174382a9ff7d7214944824",
      "40846c004b2840dcbbafc89ccddae841",
      "33c2c04e35744da193e167850aa6071b",
      "053f04bef2844743b18ff394629c8dd6",
      "db73bf7c322542698395ceba2124fd25",
      "743ea4f18384448e8055af108546807a",
      "d868679579904e95aab1671b9f888c6d",
      "bdd1e6b0df404ac3a0273b435801b2a7",
      "69ab88246bfa4b4bb2f05a5b126266b3"
     ]
    },
    "id": "LMRYRCtkHsFu",
    "outputId": "1b42f0ae-cd1f-4989-9f88-0d7f93ce6e26"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4e6cbb86084d4993af25f6db3a022a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [19250,\n",
       "  248,\n",
       "  14413,\n",
       "  17076,\n",
       "  28832,\n",
       "  22729,\n",
       "  14476,\n",
       "  21982,\n",
       "  19329,\n",
       "  19610,\n",
       "  14442,\n",
       "  14048,\n",
       "  19441,\n",
       "  18555,\n",
       "  29874,\n",
       "  12024,\n",
       "  19250,\n",
       "  248,\n",
       "  14413,\n",
       "  17076,\n",
       "  28832,\n",
       "  298,\n",
       "  13468,\n",
       "  15872,\n",
       "  29874,\n",
       "  14307,\n",
       "  17389,\n",
       "  11699,\n",
       "  29708,\n",
       "  11821,\n",
       "  14680,\n",
       "  18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  19250,\n",
       "  248,\n",
       "  14413,\n",
       "  17076,\n",
       "  15326,\n",
       "  15155,\n",
       "  22729,\n",
       "  14058,\n",
       "  14182,\n",
       "  250,\n",
       "  15074,\n",
       "  14129,\n",
       "  27450,\n",
       "  25438,\n",
       "  14379,\n",
       "  16259,\n",
       "  12024,\n",
       "  27331,\n",
       "  12273,\n",
       "  10487,\n",
       "  12007,\n",
       "  16890,\n",
       "  16608,\n",
       "  24374,\n",
       "  10524,\n",
       "  12007,\n",
       "  14623,\n",
       "  14781,\n",
       "  16601,\n",
       "  14253,\n",
       "  14130,\n",
       "  27331,\n",
       "  12273,\n",
       "  10487,\n",
       "  12005,\n",
       "  14641,\n",
       "  21901,\n",
       "  16495,\n",
       "  14232,\n",
       "  17581,\n",
       "  14488,\n",
       "  9499,\n",
       "  16975,\n",
       "  14053,\n",
       "  10839,\n",
       "  12037,\n",
       "  14025,\n",
       "  22676,\n",
       "  13125,\n",
       "  18658,\n",
       "  21663,\n",
       "  18188,\n",
       "  15979,\n",
       "  29262,\n",
       "  1700,\n",
       "  321,\n",
       "  14879,\n",
       "  310,\n",
       "  14524,\n",
       "  16452,\n",
       "  14374,\n",
       "  14696,\n",
       "  14363,\n",
       "  14370,\n",
       "  27656,\n",
       "  19294,\n",
       "  14979,\n",
       "  26392,\n",
       "  14182,\n",
       "  252,\n",
       "  14515,\n",
       "  14129,\n",
       "  14650,\n",
       "  16890,\n",
       "  14049,\n",
       "  14374,\n",
       "  18725,\n",
       "  14432,\n",
       "  15964,\n",
       "  18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  14182,\n",
       "  250,\n",
       "  15074,\n",
       "  14129,\n",
       "  27450,\n",
       "  25438,\n",
       "  14379,\n",
       "  26469,\n",
       "  27331,\n",
       "  12273,\n",
       "  10487,\n",
       "  12007,\n",
       "  16890,\n",
       "  13594,\n",
       "  14491,\n",
       "  16247,\n",
       "  15002,\n",
       "  27450,\n",
       "  25438,\n",
       "  20138,\n",
       "  14413,\n",
       "  15892,\n",
       "  21009,\n",
       "  24015,\n",
       "  14257,\n",
       "  15070,\n",
       "  11786,\n",
       "  15055,\n",
       "  23561,\n",
       "  14979,\n",
       "  15527,\n",
       "  15173,\n",
       "  27450,\n",
       "  14903,\n",
       "  21660,\n",
       "  18043,\n",
       "  14134,\n",
       "  16516,\n",
       "  14871,\n",
       "  24538,\n",
       "  21190,\n",
       "  14157,\n",
       "  15573,\n",
       "  13607,\n",
       "  15516,\n",
       "  282,\n",
       "  270,\n",
       "  19911,\n",
       "  15206,\n",
       "  14191,\n",
       "  16119,\n",
       "  14130,\n",
       "  15720,\n",
       "  18041,\n",
       "  14182,\n",
       "  250,\n",
       "  15074,\n",
       "  14129,\n",
       "  14256,\n",
       "  373,\n",
       "  12013,\n",
       "  9230,\n",
       "  12080,\n",
       "  14871,\n",
       "  11786,\n",
       "  18383,\n",
       "  14537,\n",
       "  26222,\n",
       "  24809,\n",
       "  19329,\n",
       "  10338,\n",
       "  18383,\n",
       "  16399,\n",
       "  268,\n",
       "  21395,\n",
       "  16399,\n",
       "  15265,\n",
       "  16556,\n",
       "  20775,\n",
       "  16203,\n",
       "  15516,\n",
       "  16203,\n",
       "  22648,\n",
       "  313,\n",
       "  16968,\n",
       "  23249,\n",
       "  19610,\n",
       "  15309,\n",
       "  16890,\n",
       "  13594,\n",
       "  14491,\n",
       "  16247,\n",
       "  27450,\n",
       "  11319,\n",
       "  16970,\n",
       "  18154,\n",
       "  14560,\n",
       "  12048,\n",
       "  29874,\n",
       "  14067,\n",
       "  13714,\n",
       "  10675,\n",
       "  14038,\n",
       "  14379,\n",
       "  16259,\n",
       "  14590,\n",
       "  27331,\n",
       "  14219,\n",
       "  12344,\n",
       "  373,\n",
       "  13718,\n",
       "  11908,\n",
       "  373,\n",
       "  12123,\n",
       "  12074,\n",
       "  14702,\n",
       "  27865,\n",
       "  15929,\n",
       "  16343,\n",
       "  14059,\n",
       "  18188,\n",
       "  21901,\n",
       "  14209,\n",
       "  27718,\n",
       "  26388,\n",
       "  29220,\n",
       "  10386,\n",
       "  15824,\n",
       "  16343,\n",
       "  22356,\n",
       "  15929,\n",
       "  16496,\n",
       "  17387,\n",
       "  18741,\n",
       "  14382,\n",
       "  15055,\n",
       "  14634,\n",
       "  21158,\n",
       "  27450,\n",
       "  11319,\n",
       "  16970,\n",
       "  18629,\n",
       "  14339,\n",
       "  14379,\n",
       "  16259,\n",
       "  14275,\n",
       "  14106,\n",
       "  11265,\n",
       "  13737,\n",
       "  14382,\n",
       "  16209,\n",
       "  13594,\n",
       "  14089,\n",
       "  26388,\n",
       "  12024,\n",
       "  14267,\n",
       "  18720,\n",
       "  15345,\n",
       "  14027,\n",
       "  15785,\n",
       "  16343,\n",
       "  14059,\n",
       "  14025,\n",
       "  22676,\n",
       "  27718,\n",
       "  18188,\n",
       "  12034,\n",
       "  14467,\n",
       "  15863,\n",
       "  25466,\n",
       "  21901,\n",
       "  29262,\n",
       "  14478,\n",
       "  14182,\n",
       "  252,\n",
       "  14515,\n",
       "  14129,\n",
       "  16172,\n",
       "  14157,\n",
       "  16890,\n",
       "  13594,\n",
       "  14491,\n",
       "  16247,\n",
       "  14322,\n",
       "  27424,\n",
       "  21009,\n",
       "  24015,\n",
       "  17931,\n",
       "  15107,\n",
       "  18266,\n",
       "  16762,\n",
       "  27245,\n",
       "  15054,\n",
       "  266,\n",
       "  269,\n",
       "  267,\n",
       "  12024,\n",
       "  22033,\n",
       "  11699,\n",
       "  14048,\n",
       "  14725,\n",
       "  18898,\n",
       "  12024,\n",
       "  14746,\n",
       "  14623,\n",
       "  22033,\n",
       "  24716,\n",
       "  23689,\n",
       "  21009,\n",
       "  24015,\n",
       "  14342,\n",
       "  16614,\n",
       "  16305,\n",
       "  15128,\n",
       "  16905,\n",
       "  15532,\n",
       "  15095,\n",
       "  20725,\n",
       "  16859,\n",
       "  14280,\n",
       "  15180,\n",
       "  11270,\n",
       "  12141,\n",
       "  14048,\n",
       "  18043,\n",
       "  17190,\n",
       "  11863,\n",
       "  17887,\n",
       "  15111,\n",
       "  15630,\n",
       "  15615,\n",
       "  15054,\n",
       "  266,\n",
       "  269,\n",
       "  267,\n",
       "  9698,\n",
       "  14618,\n",
       "  22900,\n",
       "  14636,\n",
       "  15418,\n",
       "  282,\n",
       "  265,\n",
       "  14039,\n",
       "  17655,\n",
       "  29615,\n",
       "  13590,\n",
       "  15365,\n",
       "  16762,\n",
       "  16428,\n",
       "  18520,\n",
       "  14130,\n",
       "  18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  14611,\n",
       "  18901,\n",
       "  14256,\n",
       "  373,\n",
       "  12013,\n",
       "  9230,\n",
       "  12080,\n",
       "  14061,\n",
       "  15068,\n",
       "  21924,\n",
       "  15247,\n",
       "  28630,\n",
       "  27865,\n",
       "  26795,\n",
       "  16390,\n",
       "  21692,\n",
       "  11786,\n",
       "  15842,\n",
       "  14049,\n",
       "  25735,\n",
       "  15042,\n",
       "  26061,\n",
       "  12080,\n",
       "  26236,\n",
       "  11207,\n",
       "  14143,\n",
       "  11771,\n",
       "  13358,\n",
       "  11775,\n",
       "  12899,\n",
       "  1543,\n",
       "  14590,\n",
       "  14032,\n",
       "  10314,\n",
       "  14624,\n",
       "  24809,\n",
       "  11786,\n",
       "  9545,\n",
       "  14404,\n",
       "  15055,\n",
       "  21366,\n",
       "  14491,\n",
       "  16247,\n",
       "  14153,\n",
       "  27450,\n",
       "  25438,\n",
       "  11786,\n",
       "  14152,\n",
       "  14880,\n",
       "  25005,\n",
       "  13497,\n",
       "  12332,\n",
       "  14057,\n",
       "  16396,\n",
       "  14454,\n",
       "  17293,\n",
       "  18862,\n",
       "  13607,\n",
       "  27331,\n",
       "  10795,\n",
       "  12790,\n",
       "  15979,\n",
       "  20778,\n",
       "  14159,\n",
       "  15090,\n",
       "  14058,\n",
       "  16816,\n",
       "  11806,\n",
       "  16067,\n",
       "  29798,\n",
       "  13594,\n",
       "  14032,\n",
       "  14821,\n",
       "  21701,\n",
       "  17840,\n",
       "  14057,\n",
       "  16396,\n",
       "  18862,\n",
       "  14049,\n",
       "  14048,\n",
       "  18043,\n",
       "  14134,\n",
       "  16516,\n",
       "  14871,\n",
       "  12007,\n",
       "  14185,\n",
       "  14641,\n",
       "  17939,\n",
       "  17212,\n",
       "  11802,\n",
       "  11877,\n",
       "  14130,\n",
       "  18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  25700,\n",
       "  13590,\n",
       "  15390,\n",
       "  10512,\n",
       "  15733,\n",
       "  16614,\n",
       "  16305,\n",
       "  14884,\n",
       "  15062,\n",
       "  16475,\n",
       "  25142,\n",
       "  13714,\n",
       "  14624,\n",
       "  14641,\n",
       "  15516,\n",
       "  282,\n",
       "  270,\n",
       "  17190,\n",
       "  9866,\n",
       "  14623,\n",
       "  15615,\n",
       "  18555,\n",
       "  29874,\n",
       "  12024,\n",
       "  27331,\n",
       "  12273,\n",
       "  10487,\n",
       "  24374,\n",
       "  10524,\n",
       "  298,\n",
       "  13468,\n",
       "  15872,\n",
       "  29874,\n",
       "  15390,\n",
       "  10512,\n",
       "  28296,\n",
       "  14403,\n",
       "  25715,\n",
       "  14256,\n",
       "  373,\n",
       "  12013,\n",
       "  9230,\n",
       "  12080,\n",
       "  14145,\n",
       "  24111,\n",
       "  17369,\n",
       "  29366],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  19250,\n",
       "  248,\n",
       "  14413,\n",
       "  17076,\n",
       "  15326,\n",
       "  15155,\n",
       "  22729,\n",
       "  14058,\n",
       "  14182,\n",
       "  250,\n",
       "  15074,\n",
       "  14129,\n",
       "  27450,\n",
       "  25438,\n",
       "  14379,\n",
       "  16259,\n",
       "  12024,\n",
       "  27331,\n",
       "  12273,\n",
       "  10487,\n",
       "  12007,\n",
       "  16890,\n",
       "  16608,\n",
       "  24374,\n",
       "  10524,\n",
       "  12007,\n",
       "  14623,\n",
       "  14781,\n",
       "  16601,\n",
       "  14253,\n",
       "  15133,\n",
       "  15002,\n",
       "  27450,\n",
       "  20138,\n",
       "  14413,\n",
       "  15892,\n",
       "  21009,\n",
       "  24015,\n",
       "  14257,\n",
       "  15070,\n",
       "  11786,\n",
       "  15055,\n",
       "  23561,\n",
       "  14979,\n",
       "  15527,\n",
       "  15173,\n",
       "  27450,\n",
       "  14903,\n",
       "  21660,\n",
       "  18043,\n",
       "  14134,\n",
       "  16516,\n",
       "  14871,\n",
       "  24538,\n",
       "  21190,\n",
       "  14157,\n",
       "  15573,\n",
       "  13607,\n",
       "  15516,\n",
       "  282,\n",
       "  270,\n",
       "  19911,\n",
       "  15206,\n",
       "  14191,\n",
       "  14064,\n",
       "  13599,\n",
       "  15479,\n",
       "  18555,\n",
       "  29874,\n",
       "  12005,\n",
       "  18901,\n",
       "  14256,\n",
       "  373,\n",
       "  12013,\n",
       "  9230,\n",
       "  12080,\n",
       "  14061,\n",
       "  15068,\n",
       "  21924,\n",
       "  15247,\n",
       "  28630,\n",
       "  27865,\n",
       "  26795,\n",
       "  16390,\n",
       "  21692,\n",
       "  11786,\n",
       "  15842,\n",
       "  14049,\n",
       "  25735,\n",
       "  15042,\n",
       "  26061,\n",
       "  12080,\n",
       "  26236,\n",
       "  11207,\n",
       "  14143,\n",
       "  11771,\n",
       "  13358,\n",
       "  11775,\n",
       "  12899,\n",
       "  1543,\n",
       "  14590,\n",
       "  14032,\n",
       "  10314,\n",
       "  14624,\n",
       "  24809,\n",
       "  11786,\n",
       "  9545,\n",
       "  14404,\n",
       "  15055,\n",
       "  21366,\n",
       "  14491,\n",
       "  16247,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"document\"]\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "tokenized_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMyghRTTHuCN"
   },
   "source": [
    "## 2-5. 모델 로드 & 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "referenced_widgets": [
      "1b31761de5dd47e9bb46c27bc928c5b0",
      "a12b35e7de664f1d8b1c76c37dcd7297",
      "24d5f4c32dbe4963bb7ef39f66bc6297",
      "7a594483fafc48bcbfe714fad6952aa6",
      "e017d7758833438db86c01b638912254",
      "cdf47c912bf3494d9beee4de012b4979",
      "2052a33333884877851c9fabbd40360e",
      "6de82b0e842e401c8eb2237d996b1a23",
      "98a39f6ea4c9465891f2c59e820ebf86",
      "1dcf50e114174fe3a7ef01b57d3792f7",
      "04c61254a3d7480aba911f05c20e57d7"
     ]
    },
    "id": "fnXRQFP9Hvt1",
    "outputId": "69d7a395-f727-4ca3-856d-8f112caac281"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b31761de5dd47e9bb46c27bc928c5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-813368061.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhhj2000\u001b[0m (\u001b[33mhhj2000-hanguk-university-of-foreign-studies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251121_144317-mmv99dig</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/mmv99dig' target=\"_blank\">ethereal-gorge-6</a></strong> to <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface' target=\"_blank\">https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/mmv99dig' target=\"_blank\">https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/mmv99dig</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4874518811702728,\n",
       " 'eval_model_preparation_time': 0.0028,\n",
       " 'eval_rouge1': 35.98,\n",
       " 'eval_rouge2': 15.41,\n",
       " 'eval_rougeL': 35.57,\n",
       " 'eval_rougeLsum': 35.52,\n",
       " 'eval_runtime': 73.8373,\n",
       " 'eval_samples_per_second': 13.543,\n",
       " 'eval_steps_per_second': 3.386}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "eval_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./eval_tmp\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3vE2k6EHySt"
   },
   "source": [
    "## 2-6. 예시 출력 몇 개 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4EmZxWiHz4l",
    "outputId": "7c8a5601-d25f-4646-b4fa-53f52f23288c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[예시 #0]\n",
      "[원문 일부]\n",
      " 2021 기업시민보고서 발간…재생에너지 100% 사용 등 담아 포스코케미칼의 2021 기업시민보고서ⓒ포스코케미칼 데일리안 조인영 기자 포스코케미칼은 2021 기업시민보고서를 발간하고 2035년까지 배터리소재 사업 부문의 탄소중립을 달성하겠다는 로드맵을 공개했다고 6일 밝혔다. 탄소중립은 다양한 감축 활동을 통해 지구 온난화의 주범인 이산화탄소의 실질적인 배출량을 제로 zero 로 만드는 것을 의미하며 세계 각국 정부와 주요 기업들은 2050년까지 이를 달성하는 것을 목표로 하고 있다. 포스코케미칼은 2035년까지 배터리소재 사업 부문에서 탄소중립을 달성할 계획이다. 이는 배터리소재 선도 기업으로서 기후변화 문제 해결에 적극 나서고 주요 자동차사와 배터리 고객사들의 친환경적인 소재 생산 요구에 선제적으로 대응 ...\n",
      "\n",
      "[실제 요약]\n",
      " 포스코케미칼은 2021 기업시민보고서를 발간하고 2035년까지 배터리소재 사업 부문의 탄소중립을 달성하겠다는 로드맵을 공개했다고 6일 밝혔는데, 이는 배터리 선도 기업으로서 기후변화 문제 해결에 적극 나서고 주요 자동차사와 배터리 고객사들의 친환경적인 소재 생산 요구에 선제적으로 대응해 ESG 경쟁력을 확보하기 위함이며 포스코케미칼은 국내외 양·음극재 공장에 태양광 발전설비 도입을 확대하고 있으며 북미에 설립하는 GM과의 양극재 합작사 ‘얼티엄캠’에서도 수력 등의 재생에너지를 적극 활용할 계획이다. \n",
      "\n",
      "[모델 요약]\n",
      " 포스코케미칼은 2021 기업시민보고서를 발간하고 2035년까지 배터리소재 사업 부문의 탄소중립을 달성하겠다는 로드맵을 공개했다고 6일 밝혔으며 이 로드맵은 다양한 감축 활동을 통해 지구 온난화의 주범인 이산화탄소의 실질적인 배출량을 제로 zero 로 만드는 것을 의미하며 세계 각국 정부와 주요 기업들은 2050년까지 이를 달성하는 것을 목표로 하고 있다. \n",
      "\n",
      "================================================================================\n",
      "[예시 #1]\n",
      "[원문 일부]\n",
      " LG엔솔 3% 넘게 하락중… 이대로면 신저가 경신 美 단독공장 투자 재검토에 보호예수 해제 겹쳐 2분기 실적도 암울… 하반기엔 개선돼 ‘매수기회’ 조언도 배터리 대장주로 꼽히는 LG에너지솔루션 373220 엔솔 주가가 신저가 갱신을 눈앞에 두고 있다. 미국 애리조나 공장 투자 계획을 재검토한다는 소식이 전해진 가운데 다음달 1000만주가량의 보호예수 물량까지 해제된다는 소식에 투심이 급격히 얼어붙은 영향이다. 증권가는 LG엔솔의 하반기 실적은 긍정적으로 전망하면서도 단기간 주가 하락은 불가피하다고 보고 있다. 1일 금융투자업계에 따르면 LG엔솔은 이날 오전 10시 40분 현재 전일 대비 3.23% 1만2000원 떨어진 35만9000원에 거래되고 있다. LG엔솔 주가는 지난달 28일부터 4거래일 연속 하락 중 ...\n",
      "\n",
      "[실제 요약]\n",
      " 배에너지솔루션 373220 엔솔 주가가 미국 애리조나 공장 투자 계획을 재검토한다는 소식에 다음달 1000만주가량의 보호예수 물량까지 해제된다는 소식에 투심이 급격히 얼어붙어 신저가 갱신을 눈앞에 두고 있으며 세계 증시가 휘청이면서 힘을 못 쓰던 LG엔솔 주가는 최근 미국 투자 계획 재검토 소식이 알려지면서 직격탄을 맞았다. \n",
      "\n",
      "[모델 요약]\n",
      " 배터리 대장주로 꼽히는 LG에너지솔루션 373220 엔솔 주가가 미국 애리조나 공장 투자 계획을 재검토한다는 소식이 전해진 가운데 다음달 1000만주가량의 보호예수 물량까지 해제된다는 소식에 투심이 급격히 얼어붙은 영향으로 신저가 갱신을 눈앞에 두고 있다. \n",
      "\n",
      "================================================================================\n",
      "[예시 #2]\n",
      "[원문 일부]\n",
      " 수원 뉴스1 신웅수 기자 4일 경기 수원시의 한 공사현장 외벽에 은행 금리 안내문이 붙어 있다. 이복현 금융감독원장이 은행의 이자 장사 에 대한 경고장을 날리자 은행들이 대출 금리를 낮추고 예·적금 금리를 올리고 있다. 신한은행은 이날부터 신규 취급 주택담보대출과 전세대출 금리를 각각 최대 0.35%포인트 p 0.30%p 낮추기로 했다 앞서 농협은행은 이달 1일부터 우대금리 확대 등을 통해 신규 주담대와 전세대출 금리를 각각 0.1%p 0.2%p 인하했다. ...\n",
      "\n",
      "[실제 요약]\n",
      " 이복현 금융감독원장이 이복현 금융감독원장이 은행의 이자 장사 에 대한 경고장을 날리자 신한은행은 이날부터 신규 취급 주택담보대출과 전세대출 금리를 각각 최대 0.35%포인트 p 0.30%p 낮추기로 하는 등 은행들이 대출 금리를 낮추고 예·적금 금리를 올리고 있다. \n",
      "\n",
      "[모델 요약]\n",
      " 이복현 금융감독원장이 은행의 이자 장사 에 대한 경고장을 날리자 신한은행은 이날부터 신규 취급 주택담보대출과 전세대출 금리를 각각 최대 0.35%포인트 p 0.30%p 낮추기로 하는 등 은행들이 대출 금리를 낮추고 예·적금 금리를 올리고 있다. 앞서 농협은행은 이달 1일부터 우대금리 확대 등을 통해 신규 주담대와 전세대출을 금리를 각각 0.1%p 0.2%p 인하했다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test set에서 몇 개 샘플 골라서 실제 요약 결과 확인\n",
    "num_examples = 3\n",
    "sample_indices = list(range(min(num_examples, len(test_dataset))))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_dataset[idx]\n",
    "    input_text = example[\"document\"]\n",
    "    ref_summary = example[\"summary\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # ✅ KoBART/BART는 token_type_ids 안 씀 → 제거\n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "    # ✅ GPU 쓰는 경우 device로 이동(선택이지만 권장)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=6,          # ✅ 4 → 6\n",
    "            length_penalty=1.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    pred_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"[예시 #{idx}]\")\n",
    "    print(\"[원문 일부]\\n\", input_text[:400], \"...\\n\")\n",
    "    print(\"[실제 요약]\\n\", ref_summary, \"\\n\")\n",
    "    print(\"[모델 요약]\\n\", pred_summary, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
