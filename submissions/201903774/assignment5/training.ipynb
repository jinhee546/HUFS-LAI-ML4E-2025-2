{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn8x3DyhEkc3"
   },
   "source": [
    "## 1-1. 제목 (Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ytcwF8w-EmGm"
   },
   "outputs": [],
   "source": [
    "# Assignment 5 — Model Training (KoBART Summarization)\n",
    "#201903774 언어인지과학과 한형준\n",
    "\n",
    "#이 노트북은 `daekeun-ml/naver-news-summarization-ko` 데이터셋을 사용하여\n",
    "#KoBART(`gogamza/kobart-base-v2`) 한국어 뉴스 요약 모델을 미세조정(fine-tuning)하는 코드입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHJb9JOpEoBe"
   },
   "source": [
    "## 1-2. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxWwXRE3Eqeu",
    "outputId": "f88a8e7b-b295-4613-c077-57c016917fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "CUDA available: True\n",
      "GPU name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets sentencepiece accelerate evaluate rouge-score\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# ✅ 여기다가 넣기\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"⚠ GPU가 안 잡혔습니다. 런타임 유형을 GPU로 바꿔주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epFSQePwE0DL"
   },
   "source": [
    "## 1-3. 설정값(config) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Q-e9E1yE1mG",
    "outputId": "c4dcc6a0-3829-4902-c81d-0e0ea65081b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_DIR: /content/drive/MyDrive/boncahier/models/kobart_ko_news\n"
     ]
    }
   ],
   "source": [
    "# 데이터/모델/학습 설정\n",
    "MODEL_NAME = \"gogamza/kobart-base-v2\"   # KoBART base\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/boncahier/models/kobart_ko_news\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)  # 학습 결과(가중치, 로그)가 저장될 경로\n",
    "\n",
    "MAX_SOURCE_LENGTH = 512   # 입력 최대 토큰 길이\n",
    "MAX_TARGET_LENGTH = 128   # 요약 최대 토큰 길이\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 4      # Colab에서 부담되지 않도록 소규모 에폭\n",
    "BATCH_SIZE = 4            # GPU VRAM에 맞게 조절\n",
    "LEARNING_RATE = 3e-5\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# 빠른 실험을 위해 샘플 수 제한 (None이면 전체 사용)\n",
    "MAX_TRAIN_SAMPLES = 5000\n",
    "MAX_EVAL_SAMPLES  = 1000\n",
    "MAX_TEST_SAMPLES  = 1000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP5eMOj4E5tO"
   },
   "source": [
    "## 1-4. 데이터 로드 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOsvtIJOE9c3",
    "outputId": "b04bcaae-8a01-4ea9-9f3e-c27de4b6b320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from CSV: (22194, 2)\n",
      "                                            document  \\\n",
      "0  앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 ...   \n",
      "1  문어 랍스터 대게 갑오징어 새우 소라 등 해산물 활용 미국식 해물찜 시푸드 보일 준...   \n",
      "2  한탄바이러스 발견 노벨상 유력 후보로 자주 거론 한국을 대표하는 의학자이자 미생물학...   \n",
      "\n",
      "                                             summary  \n",
      "0  올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, ...  \n",
      "1  인터엑스 1층 뷔페 레스토랑 브래서리는 오는 6일부터 8월31일까지 쿨 섬머 페스타...  \n",
      "2  이 이호왕 고려대 명예교수는 바이러스의 병원체와 진단법 백신까지 모두 개발한 한국을...  \n",
      "After filtering: (19060, 5)\n",
      "Dataset({\n",
      "    features: ['document', 'summary', 'doc_len', 'sum_len', 'ratio'],\n",
      "    num_rows: 19060\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'doc_len', 'sum_len', 'ratio'],\n",
       "        num_rows: 15248\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'doc_len', 'sum_len', 'ratio'],\n",
       "        num_rows: 1906\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'doc_len', 'sum_len', 'ratio'],\n",
       "        num_rows: 1906\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. CSV에서 한국어 요약 데이터 로드 (Assignment 4 결과 재사용)\n",
    "#    - 파일: data/naver_news_summarization_ko.csv\n",
    "#    - 컬럼: date, category, press, title, document, link, summary\n",
    "# ============================================================\n",
    "\n",
    "csv_path = \"data/naver_news_summarization_ko.csv\"\n",
    "\n",
    "ko_df = pd.read_csv(csv_path)\n",
    "\n",
    "# 우리가 필요한 건 요약 학습용 컬럼: document(본문), summary(요약)\n",
    "ko_df = ko_df[[\"document\", \"summary\"]].dropna().reset_index(drop=True)\n",
    "print(\"Loaded from CSV:\", ko_df.shape)\n",
    "print(ko_df.head(3))\n",
    "\n",
    "def filter_examples(df):\n",
    "    df = df.copy()\n",
    "    df[\"doc_len\"] = df[\"document\"].astype(str).str.len()\n",
    "    df[\"sum_len\"] = df[\"summary\"].astype(str).str.len()\n",
    "    df[\"ratio\"] = df[\"sum_len\"] / df[\"doc_len\"]\n",
    "\n",
    "    df = df[(df.doc_len >= 200) & (df.sum_len >= 30)]\n",
    "    df = df[(df.ratio >= 0.05) & (df.ratio <= 0.7)]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "ko_df = filter_examples(ko_df)\n",
    "print(\"After filtering:\", ko_df.shape)\n",
    "\n",
    "# HuggingFace Dataset으로 변환\n",
    "raw_dataset = Dataset.from_pandas(ko_df, preserve_index=False)\n",
    "print(raw_dataset)\n",
    "\n",
    "# 셔플 후 train/valid/test로 8:1:1 분할\n",
    "raw_dataset = raw_dataset.shuffle(seed=SEED)\n",
    "\n",
    "train_valid_test = raw_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "temp = train_valid_test[\"test\"]\n",
    "valid_test = temp.train_test_split(test_size=0.5, seed=SEED)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_valid_test[\"train\"],\n",
    "    \"validation\": valid_test[\"train\"],\n",
    "    \"test\": valid_test[\"test\"],\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egpaEQfsFDyf"
   },
   "source": [
    "## 1-5. 샘플 수 제한(선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LhjuAndFFZm",
    "outputId": "bd6b6dad-70bc-4cfe-c5b5-b09dbdab3104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000\n",
      "Valid size: 1000\n",
      "Test size : 1000\n"
     ]
    }
   ],
   "source": [
    "def maybe_subsample(ds, max_samples, seed=SEED):\n",
    "    if max_samples is None or len(ds) <= max_samples:\n",
    "        return ds\n",
    "    return ds.shuffle(seed=seed).select(range(max_samples))\n",
    "\n",
    "train_dataset = maybe_subsample(dataset_dict[\"train\"], MAX_TRAIN_SAMPLES)\n",
    "eval_dataset  = maybe_subsample(dataset_dict[\"validation\"], MAX_EVAL_SAMPLES)\n",
    "test_dataset  = maybe_subsample(dataset_dict[\"test\"], MAX_TEST_SAMPLES)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Valid size:\", len(eval_dataset))\n",
    "print(\"Test size :\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lv_AAaAFHJ-"
   },
   "source": [
    "## 1-6. 토크나이저 & 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f44ba279a67f4a2b9f8c8c3e10dd474b",
      "486cc9cee9b8488c84708d1ecb253179",
      "442880bf2ec94acc91e35141269981c3",
      "0c8ec6a579b742cf8da9ade60d809025",
      "c1d37df682b84d368bb231f6e45c3454",
      "eb0c3beec133459c960c6fc30baf76ee",
      "8fe6e87680934ff6823f4fe44edae708",
      "53c08ef0a5394d4eaee3f79056f0f2f5",
      "58e86daefc8044f3bda78684b1158ba1",
      "46796978fb574fbdadec017453d87b5b",
      "1ceabab897d042e1998ee884be2491b2",
      "35151458118641d8933fee7939f6dcf4",
      "f6e1489a3f2546ee8edba15867d297fd",
      "9d05a4cf4113477db47a0c73b513e01e",
      "e941be357c414e6bae1e9b7ec1234d6e",
      "ee05135116a24726814894c48ca1dd04",
      "f937d4b525394f50a4b5b40f56c758d8",
      "640c8913f9824b2cb1849fab0f95b9c3",
      "3bb78f962ae64d59a1ae7146d7b9884f",
      "9fbde6e7e74c4acf8cec1826bef370ee",
      "0900feb5c0e14e899cb463af590974e1",
      "1467cde3ce7a46f3b6cadd8f81081bd4",
      "a0b3d51893d3424db356c56d109b3a49",
      "f06a3a6af7324d3b905ef00b525a94b0",
      "e4aaf2a429d34cdf8c3c5afe0a16971d",
      "a391a19a53164d77a4e5649befdb5dec",
      "3e8ac3bbe6644ea29eb4b5a41059bb7f",
      "928ebd47e40d4d1b875a6db59611e4cb",
      "e2c58930da4346c4a572c1b427ac454f",
      "c1e55a678204490399b483e6bf3f5757",
      "40a15efca0dd4887b3a514d3d72afd98",
      "ce0ab1c0b8c941d28d65a73888d423ea",
      "e795b9f414984f3a8705b0252d162bd4",
      "c9ae072238d6444b82d6cf7f2b2ae658",
      "2a2b0345f85f4bfba7f5a5e433161790",
      "d706f234f17e4ed0928a724542700b95",
      "26a90a27ccf14de39c43709723045aa2",
      "086e39bdaf9c454db1e1044720e1ed20",
      "d576d2b7acc049e6a4247b6852f96aad",
      "28e08e49b50e4a73aca762d39c161e74",
      "eefbe5616fc54f348a8cf9ded3a9653f",
      "bf481d433282404a90415d67f45ce01f",
      "050d1934b7db4d3ea83c08b96db32320",
      "cc1761226009450aa892e89531338918",
      "c85aa588ca97422583c55b1960869582",
      "1018b23d44b147d3bcc7ee70781b425e",
      "7db3ad62cefe4f738536a91c6b85a69d",
      "4889046fb39640bf80ebfbac0cb93f7a",
      "b1312d766df84996bcbb7dcca3aac8e2",
      "96353c3e74484acb85af619d6047d84b",
      "ffb02de0dedd47fc801b1462dfdb32f7",
      "204669fa86114f778a96cdc0a47425a5",
      "611dab3daec345a98ba5c668f35c6465",
      "de51a3a5cb374872a7fbe0ed1d64187c",
      "737d9313d38d43d78c835534667e0d6e",
      "4738f4ae84ae44499f22d5efac4abd4d",
      "b51c492b13804e50b05abbee11fee061",
      "27a5625e964b42708bcdab3bbfe39ee9",
      "ac8650d1c1404700b629eccb51970086",
      "af073329353246aa8a3b29486faea072",
      "ed40ba2848404d96bf72081bb470ac04",
      "de115f6dcec94273a793908f87622d3a",
      "3d5c71dd7e2d4d8c9d9d8ee850b1585f",
      "643c782047454f2180d30fc79f9b9913",
      "39dfbe9dfb0e43b28b2ff6a1d21fed60",
      "8819f89b0e31476496be207ce16d73f2",
      "e04e7a56f2c64709b05df46322b669ca",
      "78892d6c72384b5fa66f368fb86e6478",
      "87883759130146479aa79b6147b7eeba",
      "61950870b992483caca9eb97bdf589ba",
      "4437f7f8bd1f4a1d86e2ef9b667a845e",
      "c1d916e1fa2f4267b800c44a6b15e56f",
      "d256bbd475ff452ab19c91e4b12f3b7c",
      "c067b44039a24e18b24e55d2e004d508",
      "f154d36d18c241ebb202f7953d212ae9",
      "aa4d169a07b0448f8a6cde3dd3672fc3",
      "730f9f4558b3439c8bbb4e4df6f3f50a"
     ]
    },
    "id": "d7s3ekmWFIoe",
    "outputId": "403547eb-7e2a-4dc3-a4cf-e0a5595615ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44ba279a67f4a2b9f8c8c3e10dd474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35151458118641d8933fee7939f6dcf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b3d51893d3424db356c56d109b3a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ae072238d6444b82d6cf7f2b2ae658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85aa588ca97422583c55b1960869582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4738f4ae84ae44499f22d5efac4abd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04e7a56f2c64709b05df46322b669ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14802,\n",
       "  14040,\n",
       "  12074,\n",
       "  20169,\n",
       "  14082,\n",
       "  27914,\n",
       "  29457,\n",
       "  15906,\n",
       "  25003,\n",
       "  245,\n",
       "  365,\n",
       "  15126,\n",
       "  248,\n",
       "  14802,\n",
       "  15126,\n",
       "  248,\n",
       "  26920,\n",
       "  11911,\n",
       "  14680,\n",
       "  14141,\n",
       "  15328,\n",
       "  16356,\n",
       "  19749,\n",
       "  16099,\n",
       "  14622,\n",
       "  27914,\n",
       "  249,\n",
       "  13699,\n",
       "  9264,\n",
       "  14039,\n",
       "  11712,\n",
       "  9085,\n",
       "  16347,\n",
       "  13679,\n",
       "  19949,\n",
       "  10500,\n",
       "  306,\n",
       "  286,\n",
       "  9242,\n",
       "  14039,\n",
       "  16476,\n",
       "  14670,\n",
       "  19446,\n",
       "  17148,\n",
       "  16067,\n",
       "  14040,\n",
       "  13607,\n",
       "  15100,\n",
       "  17468,\n",
       "  14175,\n",
       "  10314,\n",
       "  19610,\n",
       "  14364,\n",
       "  21716,\n",
       "  15615,\n",
       "  14188,\n",
       "  11372,\n",
       "  10314,\n",
       "  29457,\n",
       "  27914,\n",
       "  29457,\n",
       "  22986,\n",
       "  27914,\n",
       "  249,\n",
       "  13699,\n",
       "  9264,\n",
       "  28733,\n",
       "  14145,\n",
       "  18288,\n",
       "  14152,\n",
       "  16832,\n",
       "  14280,\n",
       "  14045,\n",
       "  15541,\n",
       "  18354,\n",
       "  14058,\n",
       "  15260,\n",
       "  16969,\n",
       "  14862,\n",
       "  18531,\n",
       "  25756,\n",
       "  17167,\n",
       "  19749,\n",
       "  19042,\n",
       "  13590,\n",
       "  14289,\n",
       "  15210,\n",
       "  14098,\n",
       "  12037,\n",
       "  16476,\n",
       "  14670,\n",
       "  19446,\n",
       "  17148,\n",
       "  10952,\n",
       "  15100,\n",
       "  12790,\n",
       "  10314,\n",
       "  14854,\n",
       "  17316,\n",
       "  21716,\n",
       "  16982,\n",
       "  14068,\n",
       "  14253,\n",
       "  14130,\n",
       "  27914,\n",
       "  249,\n",
       "  13699,\n",
       "  9264,\n",
       "  16099,\n",
       "  26466,\n",
       "  14085,\n",
       "  16239,\n",
       "  14193,\n",
       "  19903,\n",
       "  14038,\n",
       "  14810,\n",
       "  27707,\n",
       "  18760,\n",
       "  15525,\n",
       "  16465,\n",
       "  12124,\n",
       "  14257,\n",
       "  18760,\n",
       "  9264,\n",
       "  14045,\n",
       "  10318,\n",
       "  26717,\n",
       "  11786,\n",
       "  16907,\n",
       "  19320,\n",
       "  12123,\n",
       "  13603,\n",
       "  29019,\n",
       "  16658,\n",
       "  14190,\n",
       "  11810,\n",
       "  14524,\n",
       "  16658,\n",
       "  14316,\n",
       "  16663,\n",
       "  14715,\n",
       "  9904,\n",
       "  14130,\n",
       "  18963,\n",
       "  18861,\n",
       "  28733,\n",
       "  18760,\n",
       "  9264,\n",
       "  14280,\n",
       "  20328,\n",
       "  28630,\n",
       "  14094,\n",
       "  13514,\n",
       "  17541,\n",
       "  14280,\n",
       "  14835,\n",
       "  14849,\n",
       "  9264,\n",
       "  14424,\n",
       "  14152,\n",
       "  16832,\n",
       "  12007,\n",
       "  18354,\n",
       "  15615,\n",
       "  27914,\n",
       "  29457,\n",
       "  15906,\n",
       "  14604,\n",
       "  14128,\n",
       "  18142,\n",
       "  14193,\n",
       "  19321,\n",
       "  14488,\n",
       "  9866,\n",
       "  19901,\n",
       "  9000,\n",
       "  13714,\n",
       "  14045,\n",
       "  10318,\n",
       "  14839,\n",
       "  20066,\n",
       "  21492,\n",
       "  19031,\n",
       "  11372,\n",
       "  10487,\n",
       "  14624,\n",
       "  19992,\n",
       "  18838,\n",
       "  14712,\n",
       "  14058,\n",
       "  15358,\n",
       "  13594,\n",
       "  14498,\n",
       "  15197,\n",
       "  20292,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [14188,\n",
       "  11372,\n",
       "  10314,\n",
       "  29457,\n",
       "  27914,\n",
       "  29457,\n",
       "  22986,\n",
       "  27914,\n",
       "  249,\n",
       "  13699,\n",
       "  9264,\n",
       "  28733,\n",
       "  14145,\n",
       "  18288,\n",
       "  14152,\n",
       "  16832,\n",
       "  14280,\n",
       "  14045,\n",
       "  15541,\n",
       "  18354,\n",
       "  14058,\n",
       "  15260,\n",
       "  16969,\n",
       "  14862,\n",
       "  18531,\n",
       "  25756,\n",
       "  17167,\n",
       "  19749,\n",
       "  19042,\n",
       "  13590,\n",
       "  14289,\n",
       "  15210,\n",
       "  14098,\n",
       "  12037,\n",
       "  16476,\n",
       "  14670,\n",
       "  19446,\n",
       "  17148,\n",
       "  10952,\n",
       "  15100,\n",
       "  12790,\n",
       "  10314,\n",
       "  14854,\n",
       "  17316,\n",
       "  21716,\n",
       "  16982,\n",
       "  14068,\n",
       "  14253,\n",
       "  14662,\n",
       "  1700,\n",
       "  14085,\n",
       "  16239,\n",
       "  14193,\n",
       "  19903,\n",
       "  14038,\n",
       "  14810,\n",
       "  27707,\n",
       "  18760,\n",
       "  15525,\n",
       "  16465,\n",
       "  12124,\n",
       "  14257,\n",
       "  18760,\n",
       "  9264,\n",
       "  14045,\n",
       "  10318,\n",
       "  26717,\n",
       "  11786,\n",
       "  16907,\n",
       "  19320,\n",
       "  12123,\n",
       "  13603,\n",
       "  29019,\n",
       "  16658,\n",
       "  14190,\n",
       "  11810,\n",
       "  14524,\n",
       "  16658,\n",
       "  14316,\n",
       "  16663,\n",
       "  14715,\n",
       "  9904,\n",
       "  14130,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "# KoBART의 경우 pad_token이 없으면 eos_token으로 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 입력: document, 타깃: summary\n",
    "    inputs = examples[\"document\"]\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # padding 토큰은 -100으로 바꿔서 loss에서 무시되도록 처리\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ9x9QUrFLem"
   },
   "source": [
    "## 1-7. 모델/데이터콜레이터/평가지표 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "0249db62e3ba4909a00e1e88ca259722",
      "3d1889945a34487fb806b6f77ae335e3",
      "cf95b744a08d4127aeb34c86293a2f4b",
      "13d276feef404b37b966fc137acdc471",
      "c9ecde31560f47d3a168a655f2e03426",
      "c0553b6b77394b34a83402916eec35c8",
      "a495724f40d34425ab3ee1e784cee40b",
      "bfa134ca0d8f44f4aca35b4a8d9a0189",
      "a686609ccf8b40f4a917e1fb5af3cac7",
      "d9c837574243431db9227363ce1304a8",
      "91a89d9b815e456a9cdb7e4cae862222",
      "506e1f8c5a9a41a7b0b605083d3c0421",
      "ad82eb3e674e443095cda5f1d3c31fbd",
      "354772fb3829416aa2524c1730255191",
      "f065c69fe21a43c0b76da710f472a04b",
      "5bb44652e3314f95bfa487ba2aaaab46",
      "bc8433c80e544fbd9d5133990dbfb713",
      "d36cbcbbe2564f098d692724a509a287",
      "3fcdd9be15e6463ab48a607d792a341a",
      "15c4619851b64e9b9555da28922e95fc",
      "a1876074b7424c9f9ae278a9593c063a",
      "4b3fa19c7eea44d4af2a88b5d4af5516"
     ]
    },
    "id": "WBUIi8g2FN5W",
    "outputId": "dcd0c33b-53ed-45ed-9af7-7197f2b8c892"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0249db62e3ba4909a00e1e88ca259722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506e1f8c5a9a41a7b0b605083d3c0421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 디코더 시작 토큰 설정 (필요 시)\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # 생성 결과 디코딩\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # 레이블 디코딩(-100 → pad_token_id)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # ROUGE 점수(0~1)를 0~100 스케일로 변환\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5_j5dyiFRB3"
   },
   "source": [
    "## 1-8. 학습 설정 & Trainer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnYCKMFwFTN3",
    "outputId": "55399a9f-865b-489a-f91c-a6f376adf998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-916859017.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    predict_with_generate=False,\n",
    "    fp16=torch.cuda.is_available(),  # GPU 있으면 mixed precision\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z18UCkBbFXMe"
   },
   "source": [
    "## 1-9. 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "QFvAz8QKFYb-",
    "outputId": "3cb49c94-0833-4246-ba0d-5b2df9266fac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhhj2000\u001b[0m (\u001b[33mhhj2000-hanguk-university-of-foreign-studies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251121_141816-ic7ea5sa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/ic7ea5sa' target=\"_blank\">crisp-breeze-5</a></strong> to <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface' target=\"_blank\">https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/ic7ea5sa' target=\"_blank\">https://wandb.ai/hhj2000-hanguk-university-of-foreign-studies/huggingface/runs/ic7ea5sa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1252' max='1252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1252/1252 09:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1252, training_loss=0.48349771103539024, metrics={'train_runtime': 597.6506, 'train_samples_per_second': 33.464, 'train_steps_per_second': 2.095, 'total_flos': 6097364582400000.0, 'train_loss': 0.48349771103539024, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhTirQ5RFZ9v"
   },
   "source": [
    "## 1-10. 최종 모델/토크나이저 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoQg60iYFb5G",
    "outputId": "0818e36f-c892-4f64-d294-3b407985c503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델과 토크나이저가 /content/drive/MyDrive/boncahier/models/kobart_ko_news 에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 최종 모델 및 토크나이저 저장\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"✅ 모델과 토크나이저가 {OUTPUT_DIR} 에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
