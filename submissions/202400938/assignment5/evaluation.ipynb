{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zxJNGOH5ewO",
        "outputId": "0c526c4a-6c77-4477-b262-ebdd7bdcaf57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Ìï≠Í≥µÏÇ¨ DB Î°úÎìú ÏôÑÎ£å\n",
            "‚úÖ ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞(Validation Set) Í∞úÏàò: 150Í∞ú\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer you are loading from '/content/drive/MyDrive/aero_bert_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e.  This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== üöÄ Ï†ÑÏ≤¥ ÌèâÍ∞Ä ÏãúÏûë (Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî) ===\n",
            "\n",
            "==================================================\n",
            "üìä Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä Î¶¨Ìè¨Ìä∏ (Validation Set)\n",
            "==================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    AIRCRAFT       0.99      0.97      0.98       333\n",
            "     AIRLINE       0.93      0.94      0.93       367\n",
            "        DATE       0.93      0.96      0.95       293\n",
            "       ROUTE       0.87      0.91      0.89       213\n",
            "\n",
            "   micro avg       0.93      0.95      0.94      1206\n",
            "   macro avg       0.93      0.95      0.94      1206\n",
            "weighted avg       0.94      0.95      0.94      1206\n",
            "\n",
            "\n",
            "==================================================\n",
            "üëÄ Ïã§Ï†ú ÏòàÏ∏° ÏÉòÌîå (ÎûúÎç§ 3Í∞ú)\n",
            "==================================================\n",
            "\n",
            "[Sample 1]\n",
            "Î¨∏Ïû•: Published at 0500PDT / 1200GMT 20OCT25Alaska Airlines in Northern summer 2026 season plans to expand...\n",
            "‚úÖ ÏôÑÎ≤ΩÌïòÍ≤å ÏòàÏ∏°Ìï®!\n",
            "\n",
            "[Sample 2]\n",
            "Î¨∏Ïû•: Published at 1701PDT 03OCT25 / 0001GMT 04OCT25Mexican carrier Aerus tomorrow (04OCT25) launches new ...\n",
            "‚úÖ ÏôÑÎ≤ΩÌïòÍ≤å ÏòàÏ∏°Ìï®!\n",
            "\n",
            "[Sample 3]\n",
            "Î¨∏Ïû•: Published at 1400PDT / 2100GMT 03OCT25Boliviana de Aviacion (BoA) in the last few days revised plann...\n",
            "‚úÖ ÏôÑÎ≤ΩÌïòÍ≤å ÏòàÏ∏°Ìï®!\n"
          ]
        }
      ],
      "source": [
        "# ========================================================\n",
        "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∞è ÏÑ§Ï†ï\n",
        "# ========================================================\n",
        "!pip install transformers seqeval datasets pandas\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "from seqeval.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n",
        "drive.mount('/content/drive')\n",
        "MODEL_PATH = '/content/drive/MyDrive/aero_bert_model'\n",
        "\n",
        "# ========================================================\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ (TrainingÍ≥º ÎèôÏùºÌïú Î°úÏßÅ)\n",
        "# ========================================================\n",
        "# 2-1. Ìï≠Í≥µÏÇ¨ DB Î°úÎìú (ÌèâÍ∞Ä Îç∞Ïù¥ÌÑ∞ ÎùºÎ≤®ÎßÅÏö©)\n",
        "try:\n",
        "    airline_df = pd.read_csv('data/airlines_list.csv', encoding='cp949')\n",
        "    full_airline_list = airline_df['ÏòÅÎ¨∏Ìï≠Í≥µÏÇ¨Î™Ö'].dropna().astype(str).tolist()\n",
        "    refined_airlines = set()\n",
        "    for name in full_airline_list:\n",
        "        parts = name.split()\n",
        "        if len(parts) > 0:\n",
        "            refined_airlines.add(parts[0])\n",
        "            refined_airlines.add(name)\n",
        "    stop_words = {'Air', 'Airlines', 'Airways', 'Aviation', 'International', 'Limited', 'Co.', 'Inc.', 'The'}\n",
        "    final_airline_list = [w for w in refined_airlines if w not in stop_words and len(w) > 2]\n",
        "    print(\"‚úÖ Ìï≠Í≥µÏÇ¨ DB Î°úÎìú ÏôÑÎ£å\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ìï≠Í≥µÏÇ¨ DB Î°úÎìú Ïã§Ìå® (Í∏∞Î≥∏ Î¶¨Ïä§Ìä∏ ÏÇ¨Ïö©)\")\n",
        "    final_airline_list = ['Korean', 'Asiana', 'Jeju', 'Jin', 'Cathay', 'Delta']\n",
        "\n",
        "# 2-2. Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "df = pd.read_csv('data/aviation_data.csv')\n",
        "df = df[df['source'] == 'AeroRoutes']\n",
        "\n",
        "# 2-3. ÏûêÎèô ÎùºÎ≤®ÎßÅ Ìï®Ïàò (TrainingÍ≥º ÎèôÏùº)\n",
        "def auto_label_text(text):\n",
        "    text = str(text)\n",
        "    tokens = text.split()\n",
        "    ner_tags = ['O'] * len(tokens)\n",
        "\n",
        "    date_pattern = r'^\\d{1,2}[A-Z]{3}\\d{2}$'\n",
        "    aircraft_pattern = r'^(A3\\d{2}|B7\\d{2}|7\\d{7}|[A-Z]?\\d{3}[a-z]?(-[A-Z0-9]+)?)$'\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        clean_token = re.sub(r'[^a-zA-Z0-9]', '', token)\n",
        "        if re.match(date_pattern, clean_token): ner_tags[i] = 'B-DATE'\n",
        "        elif re.match(aircraft_pattern, clean_token): ner_tags[i] = 'B-AIRCRAFT'\n",
        "        elif ('‚Äì' in token or '-' in token) and len(token) > 3 and not re.match(aircraft_pattern, clean_token): ner_tags[i] = 'I-ROUTE'\n",
        "        elif clean_token in final_airline_list: ner_tags[i] = 'B-AIRLINE'\n",
        "    return tokens, ner_tags\n",
        "\n",
        "data = []\n",
        "for content in df['content']:\n",
        "    tokens, tags = auto_label_text(content)\n",
        "    data.append({'tokens': tokens, 'ner_tags': tags})\n",
        "\n",
        "# 2-4. ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÌï† (Validation SetÎßå ÏÇ¨Ïö©)\n",
        "_, val_data = train_test_split(data, test_size=0.15, random_state=42)\n",
        "print(f\"‚úÖ ÌèâÍ∞ÄÏö© Îç∞Ïù¥ÌÑ∞(Validation Set) Í∞úÏàò: {len(val_data)}Í∞ú\")\n",
        "\n",
        "# ========================================================\n",
        "# 3. Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
        "# ========================================================\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
        "model = BertForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ÎùºÎ≤® ÎßµÌïë Ï†ïÎ≥¥ Î°úÎìú\n",
        "id2label = model.config.id2label\n",
        "label2id = model.config.label2id\n",
        "label_list = list(label2id.keys()) # ['B-AIRCRAFT', 'B-AIRLINE', ...]\n",
        "\n",
        "# ========================================================\n",
        "# 4. ÌèâÍ∞Ä Ïã§Ìñâ (Ï†ÑÏ≤¥ Validation Set)\n",
        "# ========================================================\n",
        "print(\"\\n=== üöÄ Ï†ÑÏ≤¥ ÌèâÍ∞Ä ÏãúÏûë (Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî) ===\")\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# Î∞∞Ïπò Îã®ÏúÑ ÎßêÍ≥† ÌïòÎÇòÏî© Ï†ïÌôïÌïòÍ≤å ÌèâÍ∞Ä\n",
        "with torch.no_grad():\n",
        "    for item in val_data:\n",
        "        tokens = item['tokens']\n",
        "        tags = item['ner_tags'] # Ï†ïÎãµ ÎùºÎ≤®\n",
        "\n",
        "        # ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï (Ï†ïÎãµ ÎùºÎ≤® ÏúÑÏπò ÎßûÏ∂îÍ∏∞)\n",
        "        tokenized_inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "        inputs = tokenized_inputs.to(device)\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "        # [ÌïµÏã¨] Subword ÌÜ†ÌÅ∞ÏùÑ Îã§Ïãú Îã®Ïñ¥ Îã®ÏúÑ ÎùºÎ≤®Î°ú Î≥ÄÌôò\n",
        "        # ÌäπÏàò ÌÜ†ÌÅ∞([CLS], [SEP])Ïù¥ÎÇò Subword(##)Îäî Ï†úÏô∏ÌïòÍ≥† Ï≤´ ÌÜ†ÌÅ∞Ïùò ÏòàÏ∏°Í∞íÎßå Í∞ÄÏ†∏Ïò¥\n",
        "        curr_pred = []\n",
        "        curr_true = []\n",
        "\n",
        "        previous_word_idx = None\n",
        "        for idx, word_idx in enumerate(word_ids):\n",
        "            if word_idx is None: continue # ÌäπÏàò ÌÜ†ÌÅ∞ Î¨¥Ïãú\n",
        "            if word_idx != previous_word_idx: # ÏÉà Îã®Ïñ¥Ïùò ÏãúÏûë\n",
        "                # ÏòàÏ∏°Í∞í Ï†ÄÏû•\n",
        "                pred_tag_id = predictions[0][idx].item()\n",
        "                curr_pred.append(id2label[pred_tag_id])\n",
        "                # Ïã§Ï†úÍ∞í Ï†ÄÏû•\n",
        "                curr_true.append(tags[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        true_labels.append(curr_true)\n",
        "        pred_labels.append(curr_pred)\n",
        "\n",
        "# ========================================================\n",
        "# 5. Í≤∞Í≥º Ï∂úÎ†• (Classification Report)\n",
        "# ========================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä Î¶¨Ìè¨Ìä∏ (Validation Set)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# seqeval ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏïåÏïÑÏÑú F1, Precision, Recall Í≥ÑÏÇ∞Ìï¥Ï§çÎãàÎã§.\n",
        "report = classification_report(true_labels, pred_labels)\n",
        "print(report)\n",
        "\n",
        "# ========================================================\n",
        "# 6. Ï†ïÏÑ± ÌèâÍ∞Ä (Ïã§Ï†ú ÏòàÏ∏° ÏÉòÌîå ÌôïÏù∏)\n",
        "# ========================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üëÄ Ïã§Ï†ú ÏòàÏ∏° ÏÉòÌîå (ÎûúÎç§ 3Í∞ú)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for i in range(3):\n",
        "    sample = val_data[i]\n",
        "    text = \" \".join(sample['tokens'])\n",
        "    true = sample['ner_tags']\n",
        "    pred = pred_labels[i]\n",
        "\n",
        "    print(f\"\\n[Sample {i+1}]\")\n",
        "    print(f\"Î¨∏Ïû•: {text[:100]}...\") # ÎÑàÎ¨¥ Í∏∞ÎãàÍπå ÏïûÎ∂ÄÎ∂ÑÎßå\n",
        "\n",
        "    # ÌãÄÎ¶∞ Î∂ÄÎ∂ÑÎßå Ï∞æÏïÑÏÑú Î≥¥Ïó¨Ï£ºÍ∏∞ (ÏóÜÏúºÎ©¥ ÏôÑÎ≤Ω)\n",
        "    diffs = []\n",
        "    for t, p, token in zip(true, pred, sample['tokens']):\n",
        "        if t != p and t != 'O': # Ï†ïÎãµÏù¥ ÏûàÎäîÎç∞ ÌãÄÎ¶∞ Í≤ΩÏö∞\n",
        "            diffs.append(f\"{token}(Ï†ïÎãµ:{t} -> ÏòàÏ∏°:{p})\")\n",
        "\n",
        "    if not diffs:\n",
        "        print(\"‚úÖ ÏôÑÎ≤ΩÌïòÍ≤å ÏòàÏ∏°Ìï®!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Ï∞®Ïù¥Ï†ê: {', '.join(diffs)}\")"
      ]
    }
  ]
}